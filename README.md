# MREB
Multimodal Reasoning and Ethics Benchmark (MREB) is an open source benchmark designed to assess the cross-domain capabilities of AI models. Currently I am comparing local llms via ollama

### Resources:
- Blog post: [Link](https://saiyashwanth.tech/mreb)
- Youtube Video: tbd 



MREB provides a standardized way to evaluate LLMs by:
- Testing cross-domain capabilities
- Providing consistent scoring metrics
- Enabling fair comparison between different models
- Focusing on practical, real-world applications

The fun part? All of the llms are run locally on my pc. Check more about my pc building experience [here](https://saiyashwanth.tech/pcbuild)

## üèÜ Leaderboard Structure

The leaderboard displays:
```
Model Name | Code Score | Logic Score | Ethics Score | Multimodal Score | Overall Score
```


## Todo
- [x] Add code tasks
- [x] Add multimodal tasks
- [x] Add more tasks in logical and ethics
- [x] Evaluation script should account for multiple categories.
- [x] Add readme for each category.
- [ ] Blog must include graphs and charts along with leaderboards.
- [ ] Add logic to write the results in a file
- [ ] Add logic to automatically create visuals using plotly

## üìù License

MREB is released under the MIT License. See the LICENSE file for more details.

## üì¨ Contact

For questions or suggestions, please open an issue or contact me - taddishetty34@gmail.com